# Import libraries
import os
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans

# Define some parameters
input_path = "C:\\python\\autoindex\\txt_output" # input directory for text files
output_path = "C:\\python\\autoindex\\category" # output directory for cluster labels
n_clusters = 3 # number of clusters to form using K-means

# Define some helper functions

def read_files(path):
    # Read all text files from a given path and return a list of file names and contents
    file_names = []
    file_contents = []
    for file in os.listdir(path):
        if file.endswith(".txt"):
            file_names.append(file)
            with open(os.path.join(path, file), encoding="utf8", errors="ignore") as f:
                file_contents.append(f.read())
    return file_names, file_contents

def print_clusters(model, feature_names, n_top_words):
    # Print the cluster labels and the top words for each cluster
    print("Cluster labels:")
    print(model.labels_)
    print()
    print("Top words per cluster:")
    order_centroids = model.cluster_centers_.argsort()[:, ::-1]
    for i in range(model.n_clusters):
        print("Cluster %d:" % i, end="")
        for ind in order_centroids[i, :n_top_words]:
            print(" %s" % feature_names[ind], end="")
        print()
        
def save_clusters(file_names, labels, path):
    # Save the cluster labels to a text file with the same filename in a given path
    for i in range(len(file_names)):
        file_name = file_names[i]
        label = labels[i]
        with open(os.path.join(path, file_name), "w") as f:
            f.write(str(label))

# Read the text files from the input directory

file_names, file_contents = read_files(input_path)

# Create a pandas dataframe to store the file names and contents

data = pd.DataFrame({"file_name": file_names, "file_content": file_contents})

# Define the specific phrases you want to categorize based on

phrases = ["request to file foreign judgment", "case number", "request for summons"]

# Create a document-term matrix using CountVectorizer with the phrases as vocabulary and case sensitivity
vectorizer = CountVectorizer(vocabulary=phrases, lowercase=False)
X = vectorizer.fit_transform(data["file_content"])

# Apply K-means clustering to the document-term matrix

km = KMeans(n_clusters=n_clusters, random_state=0)
km.fit(X)

# Print the cluster labels and the top words for each cluster to the console

feature_names = vectorizer.get_feature_names_out()
print_clusters(km, feature_names, len(phrases))

# Save the cluster labels to a text file with the same filename in the output directory

save_clusters(data["file_name"], km.labels_, output_path)